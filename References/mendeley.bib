@article{Liu2018,
abstract = {We present a new algorithm to train a robust neural network against adversarial attacks. Our algorithm is motivated by the following two ideas. First, although recent work has demonstrated that fusing randomness can improve the robustness of neural networks (Liu 2017), we noticed that adding noise blindly to all the layers is not the optimal way to incorporate randomness. Instead, we model randomness under the framework of Bayesian Neural Network (BNN) to formally learn the posterior distribution of models in a scalable way. Second, we formulate the mini-max problem in BNN to learn the best model distribution under adversarial attacks, leading to an adversarial-trained Bayesian neural net. Experiment results demonstrate that the proposed algorithm achieves state-of-the-art performance under strong attacks. On CIFAR-10 with VGG network, our model leads to 14$\backslash${\%} accuracy improvement compared with adversarial training (Madry 2017) and random self-ensemble (Liu 2017) under PGD attack with {\$}0.035{\$} distortion, and the gap becomes even larger on a subset of ImageNet.},
archivePrefix = {arXiv},
arxivId = {1810.01279},
author = {Liu, Xuanqing and Li, Yao and Wu, Chongruo and Hsieh, Cho-Jui},
eprint = {1810.01279},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - Adv-BNN Improved Adversarial Defense through Robust Bayesian Neural Network.pdf:pdf},
month = {oct},
title = {{Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network}},
url = {http://arxiv.org/abs/1810.01279},
year = {2018}
}
@article{Nguyen2017,
abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
archivePrefix = {arXiv},
arxivId = {1710.10628},
author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
eprint = {1710.10628},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2017 - Variational Continual Learning.pdf:pdf},
month = {oct},
title = {{Variational Continual Learning}},
url = {http://arxiv.org/abs/1710.10628},
year = {2017}
}
@article{DBLP:journals/corr/SzegedyLJSRAEVR14,
abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott E and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {1409.4842},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
journal = {CoRR},
month = {sep},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842},
volume = {abs/1409.4},
year = {2014}
}
@article{DBLP:journals/corr/SzegedyLJSRAEVR14,
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott E and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
eprint = {1409.4842},
journal = {CoRR},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842},
volume = {abs/1409.4},
year = {2014}
}
@article{DBLP:journals/corr/SzegedyIV16,
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent},
eprint = {1602.07261},
journal = {CoRR},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
volume = {abs/1602.0},
year = {2016}
}
@incollection{NIPS2012_4824,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
doi = {10.1201/9781420010749},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781420010749},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{DBLP:journals/corr/SzegedyVISW15,
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
eprint = {1512.00567},
journal = {CoRR},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
volume = {abs/1512.0},
year = {2015}
}
@article{DBLP:journals/corr/SzegedyVISW15,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision(3).pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {CoRR},
month = {dec},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
volume = {abs/1512.0},
year = {2015}
}
@incollection{NIPS2012_4824,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@inproceedings{Szegedy2016,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1409.4842v1},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1409.4842v1},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {https://arxiv.org/pdf/1409.4842.pdf},
volume = {2016-Decem},
year = {2016}
}
@article{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropa...},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
doi = {10.1162/neco.1989.1.4.541},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Recognition.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {dec},
number = {4},
pages = {541--551},
publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1989.1.4.541},
volume = {1},
year = {1989}
}
@techreport{Hennig2012a,
abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
author = {Hennig, Philipp and {Schuler CHRISTIANSCHULER}, Christian J},
booktitle = {Journal of Machine Learning Research},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hennig, Schuler CHRISTIANSCHULER - 2012 - Entropy Search for Information-Efficient Global Optimization(4).pdf:pdf},
keywords = {Gaussian processes,expectation propagation,information,optimization,probability},
pages = {1809--1837},
title = {{Entropy Search for Information-Efficient Global Optimization}},
url = {http://www.jmlr.org/papers/volume13/hennig12a/hennig12a.pdf},
volume = {13},
year = {2012}
}
@article{Ginsbourger2008,
abstract = {The optimization of expensive-to-evaluate functions generally relies on metamodel-based exploration strategies. Many deterministic global optimization algorithms used in the field of computer experiments are based on Kriging (Gaussian process regression). Starting with a spatial predictor including a measure of uncertainty, they proceed by iteratively choosing the point maximizing a criterion which is a compromise between predicted performance and uncertainty. Distributing the evaluation of such numerically expensive objective functions on many processors is an appealing idea. Here we investigate a multi-points optimization criterion, the multipoints expected improvement (q-EI), aimed at choosing several points at the same time. An analytical expression of the q-EI is given when q = 2, and a consistent statistical estimate is given for the general case. We then propose two classes of heuristic strategies meant to approximately optimize the q-EI, and apply them to Gaussian Processes and to the classical Branin-Hoo test-case function. It is finally demonstrated within the covered example that the latter strategies perform as good as the best Latin Hypercubes and Uniform Designs ever found by simulation (2000 designs drawn at random for every q in 1, 10).},
author = {Ginsbourger, David and Riche, Rodolphe Le and Carraro, Laurent},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ginsbourger, Le Riche, Carraro - 2008 - A Multi-points Criterion for Deterministic Parallel Global Optimization based on Gaussian Proces.pdf:pdf},
journal = {In Intl. Conf. on Nonconvex Programming, NCP07, page ..., Rouen, France.},
keywords = {active learning,ego,expected improvement,kriging,monte-carlo},
pages = {1--30},
title = {{A multi-points criterion for deterministic parallel global optimization based on Gaussian processes}},
url = {https://hal.archives-ouvertes.fr/hal-00260579 http://hal-emse.ccsd.cnrs.fr/hal-00260579/},
year = {2008}
}
@techreport{Brochu2010b,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments-active user modelling with preferences, and hierarchical reinforcement learning-and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599v1},
author = {Brochu, Eric and Cora, Vlad M and {De Freitas}, Nando},
eprint = {1012.2599v1},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, De Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Mo(2).pdf:pdf},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {https://arxiv.org/pdf/1012.2599v1.pdf},
year = {2010}
}
@article{Elsken2017,
abstract = {Neural networks have recently had a lot of success for many tasks. However, neural network architectures that perform well are still typically designed manually by experts in a cumbersome trial-and-error process. We propose a new method to automatically search for well-performing CNN architectures based on a simple hill climbing procedure whose operators apply network morphisms, followed by short optimization runs by cosine annealing. Surprisingly, this simple method yields competitive results, despite only requiring resources in the same order of magnitude as training a single network. E.g., on CIFAR-10, our method designs and trains networks with an error rate below 6{\%} in only 12 hours on a single GPU; training for one day reduces this error further, to almost 5{\%}.},
archivePrefix = {arXiv},
arxivId = {1711.04528},
author = {Elsken, Thomas and Metzen, Jan-Hendrik and Hutter, Frank},
eprint = {1711.04528},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elsken, Metzen, Hutter - 2017 - Simple And Efficient Architecture Search for Convolutional Neural Networks.pdf:pdf},
month = {nov},
title = {{Simple And Efficient Architecture Search for Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1711.04528},
year = {2017}
}
@article{Baker2017,
abstract = {Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.},
archivePrefix = {arXiv},
arxivId = {1705.10823},
author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
eprint = {1705.10823},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baker et al. - 2017 - Accelerating Neural Architecture Search using Performance Prediction.pdf:pdf},
month = {may},
title = {{Accelerating Neural Architecture Search using Performance Prediction}},
url = {http://arxiv.org/abs/1705.10823},
year = {2017}
}
@article{Zela2018,
abstract = {While existing work on neural architecture search (NAS) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main NAS and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
archivePrefix = {arXiv},
arxivId = {1807.06906},
author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
eprint = {1807.06906},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zela et al. - 2018 - Towards Automated Deep Learning Efficient Joint Neural Architecture and Hyperparameter Search.pdf:pdf},
month = {jul},
title = {{Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search}},
url = {http://arxiv.org/abs/1807.06906},
year = {2018}
}
@article{williams1992simple,
author = {Williams, Ronald J},
journal = {Machine learning},
number = {3-4},
pages = {229--256},
publisher = {Springer},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@article{Wistuba,
abstract = {The design of neural network architectures for a new data set is a laborious task which requires human deep learning expertise. In order to make deep learning available for a broader audience, automated methods for finding a neural network architecture are vital. Recently proposed methods can already achieve human expert level performances. However, these methods have run times of months or even years of GPU computing time, ignoring hardware constraints as faced by many researchers and companies. We propose the use of Monte Carlo planning in combination with two different UCT (upper confidence bound applied to trees) derivations to search for network architectures. We adapt the UCT algorithm to the needs of network architecture search by proposing two ways of sharing information between different branches of the search tree. In an empirical study we are able to demonstrate that this method is able to find competitive networks for MNIST, SVHN and CIFAR-10 in just a single GPU day. Extending the search time to five GPU days, we are able to outperform human architectures and our competitors which consider the same types of layers.},
archivePrefix = {arXiv},
arxivId = {1712.07420},
author = {Wistuba, Martin},
eprint = {1712.07420},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wistuba - Unknown - Finding Competitive Network Architectures Within a Day Using UCT.pdf:pdf},
isbn = {1712.07420v2},
title = {{Finding Competitive Network Architectures Within a Day Using UCT}},
url = {https://arxiv.org/pdf/1712.07420.pdf http://arxiv.org/abs/1712.07420},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
month = {jul},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@misc{Baumrucker2012,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {MacAuley, Robert},
booktitle = {Journal of Pain and Symptom Management},
doi = {10.1016/S0885-3924(12)00520-9},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 1992 - Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.pdf:pdf},
issn = {18736513},
keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
number = {5},
pages = {793--794},
title = {{Pediatric palliative care after hours: Phone support for family caregivers}},
url = {https://link.springer.com/content/pdf/10.1007/BF00992696.pdf},
volume = {44},
year = {2012}
}
@inproceedings{Bergstra,
abstract = {In vitro growth and multiplication of shoots of a woody tree species Wrightia tomentosa in a controlled carbon dioxide environment was studied. The cultures were grown on BA supplemented MS medium with or without 3{\%} sucrose. A range of CO2 concentrations (0.0, 0.6, 10.0 and 40.0 g m(-3)) was controlled in small chambers by using solutions of NaHCO3, Na2CO3, KHCO3 and K2CO3. To obtain a CO2-free environment, a saturated solution of KOH was kept in the chambers. It was concluded that the growing shoot cultures required either sucrose in the medium as a carbon source or an ambient CO2 environment. Complete absence of a carbon source caused severe browning of the shoots and death within 30 days. The cultures grew better with 10.0 g m(-1) carbon dioxide in the environment than with 3.0{\%} sucrose in the medium. With both CO2 and sucrose being available, the best response as obtained at 0.6 g m(-3) CO2 in the chamber. At this concentration the rate of shoot multiplication was nearly double the standard rate obtained when exposed to the natural CO2 level and sucrose-supplemented medium. Total fresh and dry weight, leaf number and area per cluster also showed the best response under this condition.},
author = {Bergstra, James},
booktitle = {NIPS},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra et al. - Unknown - Algorithms for Hyper-Parameter Optimization.pdf:pdf},
pages = {1--9},
title = {{Algorithms for Hyper-Parameter Optimization}},
url = {https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf},
year = {2010}
}
@techreport{Jozefowicz2015,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
booktitle = {Jmlr},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1512.03385},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jozefowicz, Zaremba, Sutskever - 2015 - An Empirical Exploration of Recurrent Network Architectures.pdf:pdf},
isbn = {9781937284978},
issn = {1045-9227},
keywords = {deep learning,denoising auto-encoder,image denoising},
number = {3},
pages = {171--180},
pmid = {18267787},
title = {{An Empirical Exploration of Recurrent Network Architectures}},
url = {http://nlp.stanford.edu/{~}socherr/EMNLP2013{\_}RNTN.pdf{\%}5Cnhttp://www.aclweb.org/anthology/D13-1170{\%}5Cnhttp://aclweb.org/supplementals/D/D13/D13-1170.Attachment.pdf{\%}5Cnhttp://oldsite.aclweb.org/anthology-new/D/D13/D13-1170.pdf{\%}5Cnhttp://jmlr.org/proceedings/p},
volume = {6},
year = {2015}
}
@article{Stanley2018,
abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field's contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
doi = {10.1038/s42256-018-0006-z},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stanley et al. - 2018 - Designing neural networks through neuroevolution.pdf:pdf},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
keywords = {Computer science,Software},
month = {jan},
number = {1},
pages = {24--35},
publisher = {Nature Publishing Group},
title = {{Designing neural networks through neuroevolution}},
url = {http://www.nature.com/articles/s42256-018-0006-z},
volume = {1},
year = {2018}
}
@article{Floreano2008,
abstract = {Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
author = {Floreano, Dario and D{\"{u}}rr, Peter and Mattiussi, Claudio},
doi = {10.1007/s12065-007-0002-4},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Floreano, D{\"{u}}rr, Mattiussi - 2008 - Neuroevolution From architectures to learning.pdf:pdf},
issn = {18645909},
journal = {Evolutionary Intelligence},
keywords = {Evolution,Learning,Neural networks},
number = {1},
pages = {47--62},
title = {{Neuroevolution: From architectures to learning}},
url = {http://lis.epfl.ch},
volume = {1},
year = {2008}
}
@article{Stanley2018a,
abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field's contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
doi = {10.1038/s42256-018-0006-z},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stanley et al. - 2018 - Designing neural networks through neuroevolution.pdf:pdf},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
keywords = {Computer science,Software},
month = {jan},
number = {1},
pages = {24--35},
publisher = {Nature Publishing Group},
title = {{Designing neural networks through neuroevolution}},
url = {http://www.nature.com/articles/s42256-018-0006-z},
volume = {1},
year = {2018}
}
@techreport{PeterAngeline1994,
abstract = {Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithm's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods. Abstract Standard methods for inducing both the structure and weight values of recurrent neural networks fit an assumed class of architectures to every task. This simplification is necessary because the interactions between network structure and function are not well understood. Evolutionary computation, which includes genetic algorithms and evolutionary programming, is a population-based search method that has shown promise in such complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. This algorithm's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods.},
author = {{Peter Angeline}, J and {Gregory Saunders}, M and {Jordan Pollack}, P},
booktitle = {IEEE Transactions on Neural Networks},
doi = {10.1109/72.265960},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Angeline, Saunders, Pollack - 1993 - An Evolutionary Algorithm that Constructs Recurrent Neural Networks.pdf:pdf},
issn = {1045-9227},
number = {1},
pages = {54--65},
title = {{An evolutionary algorithm that constructs recurrent neural networks}},
url = {http://www.demo.cs.brandeis.edu/papers/ieeenn.pdf},
volume = {5},
year = {1994}
}
@techreport{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993v5},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993v5},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - Unknown - Densely Connected Convolutional Networks.pdf:pdf},
isbn = {9781538604571},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
url = {https://github.com/liuzhuang13/DenseNet.},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision(3).pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
volume = {2016-Decem},
year = {2016}
}
@inproceedings{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
url = {http://arxiv.org/abs/1512.03385},
volume = {2016-Decem},
year = {2016}
}
@article{Real,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---AmoebaNet-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-the-art 83.9{\%} / 96.6{\%} top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
archivePrefix = {arXiv},
arxivId = {1802.01548},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V},
eprint = {1802.01548},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Real et al. - Unknown - Regularized Evolution for Image Classifier Architecture Search.pdf:pdf},
title = {{Regularized Evolution for Image Classifier Architecture Search}},
url = {http://arxiv.org/abs/1802.01548},
year = {2018}
}
@article{Suganuma,
abstract = {Researchers have applied deep neural networks to image restoration tasks, in which they proposed various network architectures, loss functions, and training methods. In particular, adversarial training, which is employed in recent studies, seems to be a key ingredient to success. In this paper, we show that simple convolutional autoencoders (CAEs) built upon only standard network components, i.e., convolutional layers and skip connections, can outperform the state-of-the-art methods which employ adversarial training and sophisticated loss functions. The secret is to employ an evolutionary algorithm to automatically search for good architectures. Training optimized CAEs by minimizing the {\$}\backslashell{\_}2{\$} loss between reconstructed images and their ground truths using the ADAM optimizer is all we need. Our experimental results show that this approach achieves 27.8 dB peak signal to noise ratio (PSNR) on the CelebA dataset and 40.4 dB on the SVHN dataset, compared to 22.8 dB and 33.0 dB provided by the former state-of-the-art methods, respectively.},
archivePrefix = {arXiv},
arxivId = {1803.00370},
author = {Suganuma, Masanori and Ozay, Mete and Okatani, Takayuki},
eprint = {1803.00370},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suganuma, Ozay, Okatani - Unknown - Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionar.pdf:pdf},
title = {{Exploiting the Potential of Standard Convolutional Autoencoders for Image Restoration by Evolutionary Search}},
url = {http://arxiv.org/abs/1803.00370},
year = {2018}
}
@inproceedings{Chollet2016,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chollet - 2016 - Xception Deep Learning with Depthwise Separable Convolutions.pdf:pdf},
isbn = {9781538604571},
month = {oct},
pages = {1800--1807},
title = {{Xception: Deep learning with depthwise separable convolutions}},
url = {http://arxiv.org/abs/1610.02357},
volume = {2017-Janua},
year = {2017}
}
@article{Xie2018,
abstract = {We propose Stochastic Neural Architecture Search (SNAS), an economical end-to-end solution to Neural Architecture Search (NAS) that trains neural operation parameters and architecture distribution parameters in same round of back-propagation, while maintaining the completeness and differentiability of the NAS pipeline. In this work, NAS is reformulated as an optimization problem on parameters of a joint distribution for the search space in a cell. To leverage the gradient information in generic differentiable loss for architecture search, a novel search gradient is proposed. We prove that this search gradient optimizes the same objective as reinforcement-learning-based NAS, but assigns credits to structural decisions more efficiently. This credit assignment is further augmented with locally decomposable reward to enforce a resource-efficient constraint. In experiments on CIFAR-10, SNAS takes less epochs to find a cell architecture with state-of-the-art accuracy than non-differentiable evolution-based and reinforcement-learning-based NAS, which is also transferable to ImageNet. It is also shown that child networks of SNAS can maintain the validation accuracy in searching, with which attention-based NAS requires parameter retraining to compete, exhibiting potentials to stride towards efficient NAS on big datasets.},
archivePrefix = {arXiv},
arxivId = {1812.09926},
author = {Xie, Sirui and Zheng, Hehui and Liu, Chunxiao and Lin, Liang},
eprint = {1812.09926},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2018 - SNAS Stochastic Neural Architecture Search.pdf:pdf},
month = {dec},
title = {{SNAS: Stochastic Neural Architecture Search}},
url = {http://arxiv.org/abs/1812.09926},
year = {2018}
}
@article{Elsken2018,
abstract = {Neural Architecture Search aims at automatically finding neural architectures that are competitive with architectures designed by human experts. While recent approaches have achieved state-of-the-art predictive performance for image recognition, they are problematic under resource constraints for two reasons: (1)the neural architectures found are solely optimized for high predictive performance, without penalizing excessive resource consumption, (2) most architecture search methods require vast computational resources. We address the first shortcoming by proposing LEMONADE, an evolutionary algorithm for multi-objective architecture search that allows approximating the entire Pareto-front of architectures under multiple objectives, such as predictive performance and number of parameters, in a single run of the method. We address the second shortcoming by proposing a Lamarckian inheritance mechanism for LEMONADE which generates children networks that are warmstarted with the predictive performance of their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. The combination of these two contributions allows finding models that are on par or even outperform both hand-crafted as well as automatically-designed networks.},
archivePrefix = {arXiv},
arxivId = {1804.09081},
author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
eprint = {1804.09081},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elsken, Metzen, Hutter - 2018 - Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution.pdf:pdf},
month = {apr},
title = {{Efficient Multi-objective Neural Architecture Search via Lamarckian Evolution}},
url = {http://arxiv.org/abs/1804.09081},
year = {2019}
}
@article{Jin,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
eprint = {1806.10282},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin, Song, Hu - Unknown - AUTO-KERAS EFFICIENT NEURAL ARCHITECTURE SEARCH WITH NETWORK MORPHISM.pdf:pdf},
title = {{Auto-Keras: An Efficient Neural Architecture Search System}},
url = {http://arxiv.org/abs/1806.10282},
year = {2018}
}
@article{Cai2018,
abstract = {Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. {\$}10{\^{}}4{\$} GPU hours) makes it difficult to $\backslash$emph{\{}directly{\}} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize{\~{}}$\backslash$emph{\{}proxy{\}} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present $\backslash$emph{\{}ProxylessNAS{\}} that can $\backslash$emph{\{}directly{\}} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08$\backslash${\%} test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6{\$}\backslashtimes{\$} fewer parameters. On ImageNet, our model achieves 3.1$\backslash${\%} better top-1 accuracy than MobileNetV2, while being 1.2{\$}\backslashtimes{\$} faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.},
archivePrefix = {arXiv},
arxivId = {1812.00332},
author = {Cai, Han and Zhu, Ligeng and Han, Song},
eprint = {1812.00332},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai, Zhu, Han - 2018 - ProxylessNAS Direct Neural Architecture Search on Target Task and Hardware.pdf:pdf},
month = {dec},
title = {{ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware}},
url = {http://arxiv.org/abs/1812.00332},
year = {2018}
}
@article{Brock2017,
abstract = {Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at https://github.com/ajbrock/SMASH},
archivePrefix = {arXiv},
arxivId = {1708.05344},
author = {Brock, Andrew and Lim, Theodore and Ritchie, J. M. and Weston, Nick},
eprint = {1708.05344},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brock et al. - 2017 - SMASH One-Shot Model Architecture Search through HyperNetworks.pdf:pdf},
month = {aug},
title = {{SMASH: One-Shot Model Architecture Search through HyperNetworks}},
url = {http://arxiv.org/abs/1708.05344},
year = {2017}
}
@techreport{Bender2018,
abstract = {There is growing interest in automating neural network architecture design. Existing architecture search methods can be computationally expensive , requiring thousands of different architectures to be trained from scratch. Recent work has explored weight sharing across models to amortize the cost of training. Although previous methods reduced the cost of architecture search by orders of magnitude, they remain complex, requiring hy-pernetworks or reinforcement learning controllers. We aim to understand weight sharing for one-shot architecture search. With careful experimental analysis, we show that it is possible to efficiently identify promising architectures from a complex search space without either hypernetworks or RL.},
author = {Bender, Gabriel and Kindermans, Pieter-Jan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bender et al. - 2018 - Understanding and Simplifying One-Shot Architecture Search.pdf:pdf},
title = {{Understanding and Simplifying One-Shot Architecture Search}},
year = {2018}
}
@article{Wang,
abstract = {We present AlphaX, a fully automated agent that designs complex neural architectures from scratch. AlphaX explores the exponentially grown search space with a distributed Monte Carlo Tree Search (MCTS) and a Meta-Deep Neural Network (DNN). MCTS intrinsically improves the search efficiency by dynamically balancing the exploration and exploitation at fine-grained states, while Meta-DNN predicts the network accuracy to guide the search, and to provide an estimated reward to speed up the rollout. As the search progresses, AlphaX also generates the training data for Meta-DNN. So, the learning of Meta-DNN is end-to-end. In 14 days with only 16 GPUs (1832 samples), AlphaX found an architecture that reaches the state-of-the-art accuracies on both CIFAR-10(97.18{\%}) and ImageNet(75.5{\%} top-1 and 92.2{\%} top-5). This demonstrates up to 10x speedup over the original searching for NASNet that used 500 GPUs in 4 days (20000 samples). On NASBench-101, AlphaX demonstrates 3x and 2.8x speedup over Random Search and Regularized Evolution. Finally, we show the searched architecture improves a variety of vision applications from Neural Style Transfer, to Image Captioning and Object Detection. Our implementation is available at https://github.com/linnanwang/AlphaX-NASBench101.},
archivePrefix = {arXiv},
arxivId = {1903.11059},
author = {Wang, Linnan and Zhao, Yiyang and Jinnai, Yuu and Tian, Yuandong and Fonseca, Rodrigo},
eprint = {1903.11059},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - AlphaX eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search.pdf:pdf},
title = {{AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search}},
url = {http://arxiv.org/abs/1903.11059},
year = {2019}
}
@article{Savarese,
abstract = {We introduce a parameter sharing scheme, in which different layers of a convolutional neural network (CNN) are defined by a learned linear combination of parameter tensors from a global bank of templates. Restricting the number of templates yields a flexible hybridization of traditional CNNs and recurrent networks. Compared to traditional CNNs, we demonstrate substantial parameter savings on standard image classification tasks, while maintaining accuracy. Our simple parameter sharing scheme, though defined via soft weights, in practice often yields trained networks with near strict recurrent structure; with negligible side effects, they convert into networks with actual loops. Training these networks thus implicitly involves discovery of suitable recurrent architectures. Though considering only the design aspect of recurrent links, our trained networks achieve accuracy competitive with those built using state-of-the-art neural architecture search (NAS) procedures. Our hybridization of recurrent and convolutional networks may also represent a beneficial architectural bias. Specifically, on synthetic tasks which are algorithmic in nature, our hybrid networks both train faster and extrapolate better to test examples outside the span of the training set.},
archivePrefix = {arXiv},
arxivId = {1902.09701},
author = {Savarese, Pedro and Maire, Michael},
eprint = {1902.09701},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Savarese, Maire - Unknown - LEARNING IMPLICITLY RECURRENT CNNS THROUGH PARAMETER SHARING.pdf:pdf},
title = {{Learning Implicitly Recurrent CNNs Through Parameter Sharing}},
url = {http://arxiv.org/abs/1902.09701},
year = {2019}
}
@inproceedings{jiang2017contextual,
abstract = {This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new model called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman rank. Our algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.},
annote = {Shows that optimisation is fundimentally more simple than reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1610.09512},
author = {Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
eprint = {1610.09512},
organization = {JMLR. org},
pages = {1704--1713},
title = {{Contextual Decision Processes with Low Bellman Rank are PAC-Learnable}},
url = {http://arxiv.org/abs/1610.09512},
year = {2016}
}
@inproceedings{cortes2017adanet,
abstract = {We present new algorithms for adaptively learning artificial neural networks. Our algorithms (AdaNet) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.},
archivePrefix = {arXiv},
arxivId = {1607.01097},
author = {Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
eprint = {1607.01097},
organization = {JMLR. org},
pages = {874--883},
title = {{AdaNet: Adaptive Structural Learning of Artificial Neural Networks}},
url = {http://arxiv.org/abs/1607.01097},
year = {2016}
}
@article{zoph2016neural,
abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.},
archivePrefix = {arXiv},
arxivId = {1611.01578},
author = {Zoph, Barret and Le, Quoc V.},
eprint = {1611.01578},
journal = {arXiv preprint arXiv:1611.01578},
title = {{Neural Architecture Search with Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.01578},
year = {2016}
}
@article{liu2017hierarchical,
author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
journal = {arXiv preprint arXiv:1711.00436},
title = {{Hierarchical representations for efficient architecture search}},
year = {2017}
}
@inproceedings{xie2017genetic,
author = {Xie, Lingxi and Yuille, Alan},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1379--1388},
title = {{Genetic cnn}},
year = {2017}
}
@incollection{miikkulainen2019evolving,
author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Daniel and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Others},
booktitle = {Artificial Intelligence in the Age of Neural Networks and Brain Computing},
pages = {293--312},
publisher = {Elsevier},
title = {{Evolving deep neural networks}},
year = {2019}
}
@article{Russo2018,
abstract = {A Tutorial on Thompson Sampling},
author = {Russo, Daniel J. and {Van Roy}, Benjamin and Kazerouni, Abbas and Osband, Ian and Wen, Zheng},
doi = {10.1561/2200000070},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Russo et al. - 2018 - A Tutorial on Thompson Sampling.pdf:pdf},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
keywords = {Bayesian learning,Machine Learning,Online learning,Optimization,Reinforcement learning},
number = {1},
pages = {1--96},
publisher = {Now Publishers, Inc.},
title = {{A Tutorial on Thompson Sampling}},
url = {http://www.nowpublishers.com/article/Details/MAL-070},
volume = {11},
year = {2018}
}
@article{Thompson1933,
abstract = {IN elaborating the relations of the present conmmunication interest was not centred upon the interpretation of particular data, but grew out of a general interest in problems of research planning. From this point of view there can be no objection to the use of data, however meagre, as a guide to action required before more can be collected; although serious objection can otherwise be raised to argument based upon a small number of observations. Indeed, the fact that such objection can never be eliminated entirely-no matter how great the number of observations-suggested the possible value of seeking other modes of operation than that of taking a large number of observations before analysis or any attemipt to direct our course. This problem is more general than that treated in Section 2, and is directly con-cerned with any case where probability criteria may be established by means of which we judge whether one mode of operation is better than another in some given sense or not. Thus, if, in this sense, P is the probability estimnate that one treatment of a certain class of individuals is better than a second, as judged by data at present available, then we might take some monotone increasing function of P, sayf(p), to fix the fraction of such individuals to be treated in the first manner; until more evidence may be utilised, where 0 {\textless}{\_} fp) {\textless} 1; the remaining fraction of such individuals (1 -f(p)) to be treated in the second manner; or we may establish a probability of treatment by the two methods of f(p) and 1 -f(p), respectively. If such a discipline were adopted, even though it were not the best possible, it seems apparent that a considerable saving of individuals otherwise sacrificed to the inferior treatment might be effected. This would be imnportant in cases where either the rate of accumulation of data is slow or the individuals treated are valuable, or both. If we arbitrarily decide to eliminate the second treatment in favour of the first at this time, then the expectation of sacrifice to the inferior treatment would be (1 -P) for all subsequently treated individuals; whereas, if, for example, we take f(p) = P, the expectation of such sacrifice would be temporartly This content downloaded from 143.89.58.9 on Thu, 30 Nov 2017 08:26:26 UTC All use subject to http://about.jstor.org/terms},
author = {Thompson, William R.},
doi = {10.2307/2332286},
issn = {00063444},
journal = {Biometrika},
month = {dec},
number = {3/4},
pages = {285},
title = {{On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples}},
url = {https://www.jstor.org/stable/2332286?origin=crossref},
volume = {25},
year = {2006}
}
@article{jones1998efficient,
author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
journal = {Journal of Global optimization},
number = {4},
pages = {455--492},
publisher = {Springer},
title = {{Efficient global optimization of expensive black-box functions}},
volume = {13},
year = {1998}
}
@inproceedings{cox1992statistical,
author = {Cox, Dennis D and John, Susan},
booktitle = {[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics},
organization = {IEEE},
pages = {1241--1246},
title = {{A statistical method for global optimization}},
year = {1992}
}
@article{mockus1978toward,
author = {Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
journal = {The Application of Bayesian Methods for Seeking the Extremum},
pages = {117--128},
title = {{Toward global optimization}},
volume = {2},
year = {1978}
}
@article{kushner1964new,
author = {Kushner, Harold J},
journal = {Journal of Basic Engineering},
number = {1},
pages = {97--106},
publisher = {American Society of Mechanical Engineers},
title = {{A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise}},
volume = {86},
year = {1964}
}
@techreport{Hennig2012,
abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
author = {Hennig, Philipp and Schuler, Christian J},
booktitle = {Journal of Machine Learning Research},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hennig, Schuler CHRISTIANSCHULER - 2012 - Entropy Search for Information-Efficient Global Optimization(4).pdf:pdf},
keywords = {Gaussian processes,expectation propagation,information,optimization,probability},
pages = {1809--1837},
title = {{Entropy Search for Information-Efficient Global Optimization}},
url = {http://www.jmlr.org/papers/volume13/hennig12a/hennig12a.pdf},
volume = {13},
year = {2012}
}
@book{williams2006gaussian,
author = {Williams, Christopher K I and Rasmussen, Carl Edward},
number = {3},
publisher = {MIT Press Cambridge, MA},
title = {{Gaussian processes for machine learning}},
volume = {2},
year = {2006}
}
@phdthesis{duvenaud2014automatic,
author = {Duvenaud, David},
file = {:home/mjhutchinson/Downloads/DavidDuvenaudThesis.pdf:pdf},
school = {University of Cambridge},
title = {{Automatic model construction with Gaussian processes}},
year = {2014}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
isbn = {026218253X},
issn = {0129-0657},
number = {2},
pages = {69--106},
pmid = {15112367},
publisher = {MIT Press},
title = {{Gaussian processes for machine learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15112367},
volume = {14},
year = {2004}
}
@article{Requeima2018,
abstract = {Multi-output regression models must exploit dependencies between outputs to maximise predictive performance. The application of Gaussian processes (GPs) to this setting typically yields models that are computationally demanding and have limited representational power. We present the Gaussian Process Autoregressive Regression (GPAR) model, a scalable multi-output GP model that is able to capture nonlinear, possibly input-varying, dependencies between outputs in a simple and tractable way: the product rule is used to decompose the joint distribution over the outputs into a set of conditionals, each of which is modelled by a standard GP. GPAR's efficacy is demonstrated on a variety of synthetic and real-world problems, outperforming existing GP models and achieving state-of-the-art performance on established benchmarks.},
archivePrefix = {arXiv},
arxivId = {1802.07182},
author = {Requeima, James and Tebbutt, Will and Bruinsma, Wessel and Turner, Richard E.},
eprint = {1802.07182},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Requeima et al. - 2018 - The Gaussian Process Autoregressive Regression Model (GPAR)(2).pdf:pdf},
month = {feb},
title = {{The Gaussian Process Autoregressive Regression Model (GPAR)}},
url = {http://arxiv.org/abs/1802.07182},
year = {2018}
}
@techreport{Jones1998,
abstract = {In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome.},
author = {Jones, Donald R and Schonlau, Matthias and Welch, William J},
booktitle = {Journal of Global Optimization},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Schonlau, Welch - 1998 - Efficient Global Optimization of Expensive Black-Box Functions.pdf:pdf},
keywords = {Bayesian global optimization,Kriging,Random function,Response surface,Stochastic process,Visualization},
pages = {455--492},
title = {{Efficient Global Optimization of Expensive Black-Box Functions}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FA{\%}3A1008306431147.pdf},
volume = {13},
year = {1998}
}
@article{Mockus,
abstract = {In this paper a review of application of Bayesian approach to global and stochastic optimization of continuous multimodal functions is given. Advantages and disadvantages of Bayesian approach (average case analysis), comparing it with more usual minimax approach (worst case analysis) are discussed. New interactive version of software for global optimization is discussed. Practical multidimensional problems of global optimization are considered},
author = {Mockus, Jonas},
doi = {10.1007/BF01099263},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mockus - Unknown - Application of Bayesian Approach to Numerical Methods of Global and Stochastic Optimization.pdf:pdf},
issn = {09255001},
journal = {Journal of Global Optimization},
keywords = {Bayesian,Optimization,continuous,global,stochastic},
number = {4},
pages = {347--365},
title = {{Application of Bayesian approach to numerical methods of global and stochastic optimization}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2FBF01099263.pdf},
volume = {4},
year = {1994}
}
@techreport{Brochu2010,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments-active user modelling with preferences, and hierarchical reinforcement learning-and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599v1},
author = {Brochu, Eric and Cora, Vlad M and {De Freitas}, Nando},
eprint = {1012.2599v1},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, De Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Mo(2).pdf:pdf},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {https://arxiv.org/pdf/1012.2599.pdf},
year = {2010}
}
@article{Snoek,
abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). The tractable posterior distribution induced by the GP leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
author = {Larmarange, Joseph and Sow, Khoudia and Broqua, Christophe and Akind{\`{e}}s, Francis and Bekelynck, Anne and Kon{\'{e}}, Mariatou},
doi = {10.1016/s2468-2667(17)30214-1},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - Unknown - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
issn = {24682667},
journal = {The Lancet Public Health},
number = {12},
pages = {e540},
title = {{Social and implementation research for ending AIDS in Africa}},
url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
volume = {2},
year = {2017}
}
@inproceedings{zhang2016flash,
author = {Zhang, Yuyu and Bahadori, Mohammad Taha and Su, Hang and Sun, Jimeng},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2016 - FLASH fast Bayesian optimization for data analytic pipelines.pdf:pdf},
organization = {ACM},
pages = {2065--2074},
title = {{FLASH: fast Bayesian optimization for data analytic pipelines}},
year = {2016}
}
@article{zhong2017practical,
author = {Zhong, Zhao and Yan, Junjie and Liu, Cheng-Lin},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong, Yan, Liu - 2017 - Practical network blocks design with q-learning.pdf:pdf},
journal = {arXiv preprint arXiv:1708.05552},
title = {{Practical network blocks design with q-learning}},
year = {2017}
}
@article{Alquier2015,
abstract = {The PAC-Bayesian approach is a powerful set of techniques to derive non- asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately intractable. One may sample from it using Markov chain Monte Carlo, but this is often too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. We specialise our results to several learning tasks (classification, ranking, matrix completion),discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.},
archivePrefix = {arXiv},
arxivId = {1506.04091},
author = {Alquier, Pierre and Ridgway, James and Chopin, Nicolas},
eprint = {1506.04091},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alquier, Ridgway, Chopin - 2015 - On the properties of variational approximations of Gibbs posteriors.pdf:pdf},
title = {{On the properties of variational approximations of Gibbs posteriors}},
year = {2015}
}
@article{kandasamy2018neural,
author = {Kandasamy, Kirthevasan and Neiswanger, Willie and Schneider, Jeff and Poczos, Barnabas and Xing, Eric},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kandasamy et al. - 2018 - Neural architecture search with bayesian optimisation and optimal transport.pdf:pdf},
journal = {arXiv preprint arXiv:1802.07191},
title = {{Neural architecture search with bayesian optimisation and optimal transport}},
year = {2018}
}
@article{bergstra2013making,
author = {Bergstra, James and Yamins, Daniel and Cox, David Daniel},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra, Yamins, Cox - 2013 - Making a science of model search Hyperparameter optimization in hundreds of dimensions for vision archite.pdf:pdf},
publisher = {JMLR},
title = {{Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures}},
year = {2013}
}
@article{Hinton1993,
author = {Hinton, Geoffrey and Hinton, Geoffrey and {Van Camp}, Drew},
journal = {IN PROC. OF THE 6TH ANN. ACM CONF. ON COMPUTATIONAL LEARNING THEORY},
pages = {5----13},
title = {{Keeping Neural Networks Simple by Minimizing the Description Length of the Weights}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.44.3435},
year = {1993}
}
@inproceedings{graves2011practical,
author = {Graves, Alex},
booktitle = {Advances in neural information processing systems},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Graves - 2011 - Practical variational inference for neural networks.pdf:pdf},
pages = {2348--2356},
title = {{Practical variational inference for neural networks}},
year = {2011}
}
@inproceedings{fusi2018probabilistic,
author = {Fusi, Nicolo and Sheth, Rishit and Elibol, Melih},
booktitle = {Advances in Neural Information Processing Systems},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fusi, Sheth, Elibol - 2018 - Probabilistic matrix factorization for automated machine learning.pdf:pdf},
pages = {3352--3361},
title = {{Probabilistic matrix factorization for automated machine learning}},
year = {2018}
}
@article{kitano1990designing,
author = {Kitano, Hiroaki},
journal = {Complex systems},
number = {4},
pages = {461--476},
title = {{Designing neural networks using genetic algorithms with graph generation system}},
volume = {4},
year = {1990}
}
@inproceedings{mendoza2016towards,
author = {Mendoza, Hector and Klein, Aaron and Feurer, Matthias and Springenberg, Jost Tobias and Hutter, Frank},
booktitle = {Workshop on Automatic Machine Learning},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mendoza et al. - 2016 - Towards automatically-tuned neural networks.pdf:pdf},
pages = {58--65},
title = {{Towards automatically-tuned neural networks}},
year = {2016}
}
@inproceedings{hernandez2015probabilistic,
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Adams, Ryan},
booktitle = {International Conference on Machine Learning},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez-Lobato, Adams - 2015 - Probabilistic backpropagation for scalable learning of bayesian neural networks.pdf:pdf},
pages = {1861--1869},
title = {{Probabilistic backpropagation for scalable learning of bayesian neural networks}},
year = {2015}
}
@article{hernandez2016black,
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Li, Yingzhen and Rowland, Mark and Hern{\'{a}}ndez-Lobato, Daniel and Bui, Thang and Turner, Richard Eric},
publisher = {International Machine Learning Society},
title = {{Black-box {\$}\alpha{\$}-divergence minimization}},
year = {2016}
}
@article{mackay1992practical,
author = {MacKay, David J C},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/MacKay - 1992 - A practical Bayesian framework for backpropagation networks.pdf:pdf},
journal = {Neural computation},
number = {3},
pages = {448--472},
publisher = {MIT Press},
title = {{A practical Bayesian framework for backpropagation networks}},
volume = {4},
year = {1992}
}
@inproceedings{bui2016deep,
author = {Bui, Thang and Hern{\'{a}}ndez-Lobato, Daniel and Hernandez-Lobato, Jose and Li, Yingzhen and Turner, Richard},
booktitle = {International Conference on Machine Learning},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bui et al. - 2016 - Deep gaussian processes for regression using approximate expectation propagation.pdf:pdf},
pages = {1472--1481},
title = {{Deep gaussian processes for regression using approximate expectation propagation}},
year = {2016}
}
@inproceedings{snoek2012practical,
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Advances in neural information processing systems},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - Unknown - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
pages = {2951--2959},
title = {{Practical bayesian optimization of machine learning algorithms}},
year = {2012}
}
@article{Blundell2015,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
month = {may},
title = {{Weight Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424},
year = {2015}
}
@article{hernandez2016black,
author = {Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Li, Yingzhen and Rowland, Mark and Hern{\'{a}}ndez-Lobato, Daniel and Bui, Thang and Turner, Richard Eric},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez-Lobato et al. - 2016 - Black-box {\$}\alpha{\$}-divergence minimization.pdf:pdf},
publisher = {International Machine Learning Society},
title = {{Black-box {\$}\alpha{\$}-divergence minimization}},
year = {2016}
}
@inproceedings{jenatton2017bayesian,
author = {Jenatton, Rodolphe and Archambeau, Cedric and Gonz{\'{a}}lez, Javier and Seeger, Matthias},
booktitle = {International Conference on Machine Learning},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jenatton et al. - 2017 - Bayesian Optimization with Tree-structured Dependencies.pdf:pdf},
pages = {1655--1664},
title = {{Bayesian Optimization with Tree-structured Dependencies}},
year = {2017}
}
@article{Baker2016,
abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using {\$}Q{\$}-learning with an {\$}\backslashepsilon{\$}-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02167},
author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
doi = {10.1080/0305215042000274942},
eprint = {1611.02167},
isbn = {2857825749},
issn = {0305215X},
pmid = {593032},
title = {{Designing Neural Network Architectures using Reinforcement Learning}},
year = {2016}
}
@article{real2017large,
author = {Real, Esteban and Moore, Sherry and Selle, Andrew and Saxena, Saurabh and Suematsu, Yutaka Leon and Tan, Jie and Le, Quoc and Kurakin, Alex},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Real et al. - 2017 - Large-scale evolution of image classifiers.pdf:pdf},
journal = {arXiv preprint arXiv:1703.01041},
title = {{Large-scale evolution of image classifiers}},
year = {2017}
}
@article{zoph2016neural,
author = {Zoph, Barret and Le, Quoc V},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph, Le - 2016 - Neural architecture search with reinforcement learning.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01578},
title = {{Neural architecture search with reinforcement learning}},
year = {2016}
}
@article{Wang2006,
abstract = {In this paper we propose a generalised iterative algorithm for calculating variational Bayesian estimates for a normal mixture model and we investigate its convergence properties. It is shown theoretically that the variational Bayes estimator converges locally to the maximum likelihood estimator at the rate of O(1/n) in the large sample limit. We also demonstrate by numerical experiments that the generalised algorithm can be accelerated by a suitable choice of step size.},
author = {Wang, Bo and Titteringtony, D. M.},
doi = {10.1214/06-BA121},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Titteringtony - 2006 - Convergence properties of a general algorithm for calculating variational Bayesian estimates for a normal m.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Laplace approximation,Local convergence,Mixture model,Variational bayes},
title = {{Convergence properties of a general algorithm for calculating variational Bayesian estimates for a normal mixture model}},
year = {2006}
}
@inproceedings{snoek2012practical,
author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
booktitle = {Advances in neural information processing systems},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Larochelle, Adams - Unknown - Practical Bayesian Optimization of Machine Learning Algorithms.pdf:pdf},
pages = {2951--2959},
title = {{Practical bayesian optimization of machine learning algorithms}},
year = {2012}
}
@book{neal2012bayesian,
author = {Neal, Radford M},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neal - 2012 - Bayesian learning for neural networks.pdf:pdf},
publisher = {Springer Science {\&} Business Media},
title = {{Bayesian learning for neural networks}},
volume = {118},
year = {2012}
}
@article{stanley2002evolving,
author = {Stanley, Kenneth O and Miikkulainen, Risto},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stanley, Miikkulainen - 2002 - Evolving neural networks through augmenting topologies.pdf:pdf},
journal = {Evolutionary computation},
number = {2},
pages = {99--127},
publisher = {MIT Press},
title = {{Evolving neural networks through augmenting topologies}},
volume = {10},
year = {2002}
}
@article{Pham2018,
abstract = {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89{\%}, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65{\%}.},
archivePrefix = {arXiv},
arxivId = {1802.03268},
author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
eprint = {1802.03268},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pham et al. - 2018 - Efficient Neural Architecture Search via Parameter Sharing(2).pdf:pdf},
month = {feb},
title = {{Efficient Neural Architecture Search via Parameter Sharing}},
url = {http://arxiv.org/abs/1802.03268},
year = {2018}
}
@article{liu2018darts,
author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Simonyan, Yang - 2018 - Darts Differentiable architecture search.pdf:pdf},
journal = {arXiv preprint arXiv:1806.09055},
title = {{Darts: Differentiable architecture search}},
year = {2018}
}
@article{Swaroop2019,
abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.},
archivePrefix = {arXiv},
arxivId = {1905.02099},
author = {Swaroop, Siddharth and Nguyen, Cuong V. and Bui, Thang D. and Turner, Richard E.},
eprint = {1905.02099},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swaroop et al. - 2019 - Improving and Understanding Variational Continual Learning.pdf:pdf},
month = {may},
title = {{Improving and Understanding Variational Continual Learning}},
url = {http://arxiv.org/abs/1905.02099},
year = {2019}
}
@article{Wei,
abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as $\backslash$emph{\{}network morphism{\}} in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
archivePrefix = {arXiv},
arxivId = {1603.01670},
author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
eprint = {1603.01670},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei et al. - Unknown - Network Morphism.pdf:pdf},
title = {{Network Morphism}},
url = {http://arxiv.org/abs/1603.01670},
year = {2016}
}
@techreport{Brochu2010a,
abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments-active user modelling with preferences, and hierarchical reinforcement learning-and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
archivePrefix = {arXiv},
arxivId = {1012.2599v1},
author = {Brochu, Eric and Cora, Vlad M and {De Freitas}, Nando},
eprint = {1012.2599v1},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brochu, Cora, De Freitas - 2010 - A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Mo(2).pdf:pdf},
title = {{A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning}},
url = {https://arxiv.org/pdf/1012.2599v1.pdf},
year = {2010}
}
@misc{Kingma2015,
author = {Kingma, Durk P. and Salimans, Tim and Welling, Max},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Salimans, Welling - 2015 - Variational Dropout and the Local Reparameterization Trick.pdf:pdf},
pages = {2575--2583},
title = {{Variational Dropout and the Local Reparameterization Trick}},
url = {http://papers.nips.cc/paper/5666-variational-dropout-and-the-local-reparameterization-trick},
year = {2015}
}
@techreport{Bui,
abstract = {Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.},
archivePrefix = {arXiv},
arxivId = {1811.11206v1},
author = {Bui, Thang D and Nguyen, Cuong V and Swaroop, Siddharth and Turner, Richard E},
eprint = {1811.11206v1},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bui et al. - Unknown - Partitioned Variational Inference A unified framework encompassing federated and continual learning.pdf:pdf},
title = {{Partitioned Variational Inference: A unified framework encompassing federated and continual learning}},
url = {https://arxiv.org/pdf/1811.11206.pdf},
year = {2018}
}
@article{DBLP:journals/corr/SzegedyIV16,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {CoRR},
month = {feb},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
volume = {abs/1602.0},
year = {2016}
}
@article{Shridhar2018,
abstract = {We introduce a novel uncertainty estimation for classification tasks for Bayesian convolutional neural networks with variational inference. By normalizing the output of a Softplus function in the final layer, we estimate aleatoric and epistemic uncertainty in a coherent manner. The intractable posterior probability distributions over weights are inferred by Bayes by Backprop. Firstly, we demonstrate how this reliable variational inference method can serve as a fundamental construct for various network architectures. On multiple datasets in supervised learning settings (MNIST, CIFAR-10, CIFAR-100), this variational inference method achieves performances equivalent to frequentist inference in identical architectures, while the two desiderata, a measure for uncertainty and regularization are incorporated naturally. Secondly, we examine how our proposed measure for aleatoric and epistemic uncertainties is derived and validate it on the aforementioned datasets.},
archivePrefix = {arXiv},
arxivId = {1806.05978},
author = {Shridhar, Kumar and Laumann, Felix and Liwicki, Marcus},
eprint = {1806.05978},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shridhar, Laumann, Liwicki - 2018 - Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Var.pdf:pdf},
month = {jun},
title = {{Uncertainty Estimations by Softplus normalization in Bayesian Convolutional Neural Networks with Variational Inference}},
url = {http://arxiv.org/abs/1806.05978},
year = {2018}
}
@article{Sciuto,
abstract = {Neural Architecture Search (NAS) aims to facilitate the design of deep networks for new tasks. Existing techniques rely on two stages: searching over the architecture space and validating the best architecture. Evaluating NAS algorithms is currently solely done by comparing their results on the downstream task. While intuitive, this fails to explicitly evaluate the effectiveness of their search strategies. In this paper, we extend the NAS evaluation procedure to include the search phase. To this end, we compare the quality of the solutions obtained by NAS search policies with that of random architecture selection. We find that: (i) On average, the random policy outperforms state-of-the-art NAS algorithms; and (ii) The results and candidate rankings of NAS algorithms do not reflect the true performance of the candidate architectures. While our former finding illustrates the fact that the NAS search space has been sufficiently constrained so that random solutions yield good results, we trace the latter back to the weight sharing strategy used by state-of-the-art NAS methods. In contrast with common belief, weight sharing negatively impacts the training of good architectures, thus reducing the effectiveness of the search process. We believe that following our evaluation framework will be key to designing NAS strategies that truly discover superior architectures.},
annote = {Shows that the search space from NAS/ENAS is good, but the search method is no better/worse than random. Also that parameter sharing is couter productive.},
archivePrefix = {arXiv},
arxivId = {1902.08142},
author = {Sciuto, Christian and Yu, Kaicheng and Jaggi, Martin and Musat, Claudiu and Salzmann, Mathieu},
eprint = {1902.08142},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sciuto et al. - Unknown - Evaluating the Search Phase of Neural Architecture Search.pdf:pdf},
title = {{Evaluating the Search Phase of Neural Architecture Search}},
url = {http://arxiv.org/abs/1902.08142},
year = {2019}
}
@article{wu2018fixing,
author = {Wu, Anqi and Nowozin, Sebastian and Meeds, Edward and Turner, Richard E and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and Gaunt, Alexander L},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2018 - Fixing variational bayes Deterministic variational inference for bayesian neural networks.pdf:pdf},
journal = {arXiv preprint arXiv:1810.03958},
title = {{Fixing variational bayes: Deterministic variational inference for bayesian neural networks}},
year = {2018}
}
@techreport{Adam2019,
abstract = {Automatic methods for generating state-of-the-art neural network architectures without human experts have generated significant attention recently. This is because of the potential to remove human experts from the design loop which can reduce costs and decrease time to model deployment. Neural architecture search (NAS) techniques have improved significantly in their computational efficiency since the original NAS was proposed. This reduction in computation is enabled via weight sharing such as in Efficient Neural Architecture Search (ENAS). However, recently a body of work confirms our discovery that ENAS does not do significantly better than random search with weight sharing, contradicting the initial claims of the authors. We provide an explanation for this phenomenon by investigating the interpretability of the ENAS controller's hidden state. We are interested in seeing if the controller embeddings are predictive of any properties of the final architecture - for example, graph properties like the number of connections, or validation performance. We find models sampled from identical controller hidden states have no correlation in various graph similarity metrics. This failure mode implies the RNN controller does not condition on past architecture choices. Importantly, we may need to condition on past choices if certain connection patterns prevent vanishing or exploding gradients. Lastly, we propose a solution to this failure mode by forcing the controller's hidden state to encode pasts decisions by training it with a memory buffer of previously sampled architectures. Doing this improves hidden state interpretability by increasing the correlation controller hidden states and graph similarity metrics.},
annote = {Rebuttal (Or understaiding of more like) of the papers putting down the NAS/ENAS line to simple search space reduction and weight sharing being counter productive.},
archivePrefix = {arXiv},
arxivId = {1904.00438},
author = {Adam, George and Lorraine, Jonathan},
eprint = {1904.00438},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Adam, Lorraine - 2019 - Understanding Neural Architecture Search Techniques.pdf:pdf},
title = {{Understanding Neural Architecture Search Techniques}},
url = {http://arxiv.org/abs/1904.00438},
year = {2019}
}
@article{Jin2018,
abstract = {Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search. The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space. Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras. The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.},
archivePrefix = {arXiv},
arxivId = {1806.10282},
author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
eprint = {1806.10282},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Real et al. - Unknown - Regularized Evolution for Image Classifier Architecture Search.pdf:pdf},
month = {feb},
title = {{Auto-Keras: An Efficient Neural Architecture Search System}},
url = {http://arxiv.org/abs/1802.01548 http://arxiv.org/abs/1806.10282},
year = {2018}
}
@article{Swersky2014,
abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
archivePrefix = {arXiv},
arxivId = {1406.3896},
author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
eprint = {1406.3896},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swersky, Snoek, Adams - 2014 - Freeze-Thaw Bayesian Optimization.pdf:pdf},
month = {jun},
title = {{Freeze-Thaw Bayesian Optimization}},
url = {http://arxiv.org/abs/1406.3896},
year = {2014}
}
@inproceedings{Domhan2015,
abstract = {Deep neural networks (DNNs) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter op- timization methods have recently been shown to yield settings competitive with those found by hu- man experts, but their widespread adoption is ham- pered by the fact that they require more compu- tational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (af- ter a few steps of stochastic gradient descent) that the resulting network performs poorly and termi- nate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the first part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting ap- proach speeds up state-of-the-art hyperparameter optimization methods for DNNs roughly twofold, enabling them to find DNN settings that yield better performance than those chosen by human experts.},
author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domhan, Springenberg, Hutter - Unknown - Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of L.pdf:pdf},
isbn = {9781577357384},
issn = {10450823},
pages = {3460--3468},
title = {{Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves}},
url = {http://aad.informatik.uni-freiburg.de/papers/15-IJCAI-Extrapolation{\_}of{\_}Learning{\_}Curves.pdf},
volume = {2015-Janua},
year = {2015}
}
@article{Li,
abstract = {Neural architecture search (NAS) is a promising research direction that has the potential to replace expert-designed networks with learned, task-specific architectures. In this work, in order to help ground the empirical results in this field, we propose new NAS baselines that build off the following observations: (i) NAS is a specialized hyperparameter optimization problem; and (ii) random search is a competitive baseline for hyperparameter optimization. Leveraging these observations, we evaluate both random search with early-stopping and a novel random search with weight-sharing algorithm on two standard NAS benchmarks---PTB and CIFAR-10. Our results show that random search with early-stopping is a competitive NAS baseline, e.g., it performs at least as well as ENAS, a leading NAS method, on both benchmarks. Additionally, random search with weight-sharing outperforms random search with early-stopping, achieving a state-of-the-art NAS result on PTB and a highly competitive result on CIFAR-10. Finally, we explore the existing reproducibility issues of published NAS results. We note the lack of source material needed to exactly reproduce these results, and further discuss the robustness of published results given the various sources of variability in NAS experimental setups. Relatedly, we provide all information (code, random seeds, documentation) needed to exactly reproduce our results, and report our random search with weight-sharing results for each benchmark on two independent experimental runs.},
archivePrefix = {arXiv},
arxivId = {1902.07638},
author = {Li, Liam and Talwalkar, Ameet},
eprint = {1902.07638},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - Unknown - Random Search and Reproducibility for Neural Architecture Search.pdf:pdf},
keywords = {Additional Key Words and Phrases: Neural Architect,Automated Machine Learning,Hyperparameter Optimization},
title = {{Random Search and Reproducibility for Neural Architecture Search}},
url = {https://github.com/quark0/darts http://arxiv.org/abs/1902.07638},
year = {2019}
}
@article{negrinho2017deeparchitect,
author = {Negrinho, Renato and Gordon, Geoff},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Negrinho, Gordon - 2017 - Deeparchitect Automatically designing and training deep architectures.pdf:pdf},
journal = {arXiv preprint arXiv:1704.08792},
title = {{Deeparchitect: Automatically designing and training deep architectures}},
year = {2017}
}
@book{Hutter2018,
author = {Hutter, Frank and Kotthoff, Lars and Vanschoren, J.},
doi = {10.1007/978-3-030-05318-5},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2018 - Automatic Machine Learning Methods, Systems, Challenges.pdf:pdf},
isbn = {978-3-030-05317-8},
keywords = {Automatic Machine Learning},
pages = {250},
title = {{Automatic machine learning: methods, systems, challenges}},
url = {https://www.automl.org/wp-content/uploads/2018/12/automl{\_}book.pdf https://research.tue.nl/en/publications/automatic-machine-learning-methods-systems-challenges},
year = {2018}
}
@article{Elsken2018a,
abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
archivePrefix = {arXiv},
arxivId = {1808.05377},
author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
eprint = {1808.05377},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elsken, Metzen, Hutter - 2018 - Neural Architecture Search A Survey(2).pdf:pdf},
month = {aug},
title = {{Neural Architecture Search: A Survey}},
url = {http://arxiv.org/abs/1808.05377},
year = {2018}
}
@inproceedings{liu2018progressive,
author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and Fei-Fei, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - Progressive neural architecture search.pdf:pdf},
pages = {19--34},
title = {{Progressive neural architecture search}},
year = {2018}
}
@inproceedings{zoph2018learning,
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zoph et al. - 2018 - Learning transferable architectures for scalable image recognition.pdf:pdf},
pages = {8697--8710},
title = {{Learning transferable architectures for scalable image recognition}},
year = {2018}
}
@article{Klein2016,
abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single configuration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary configurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed Fabolas, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that Fabolas often finds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
archivePrefix = {arXiv},
arxivId = {1605.07079},
author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
eprint = {1605.07079},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Klein et al. - 2017 - Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets.pdf:pdf},
title = {{Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets}},
url = {http://arxiv.org/abs/1605.07079},
year = {2016}
}
@article{swersky2014raiders,
author = {Swersky, Kevin and Duvenaud, David and Snoek, Jasper and Hutter, Frank and Osborne, Michael A},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Swersky et al. - 2014 - Raiders of the lost architecture Kernels for Bayesian optimization in conditional parameter spaces.pdf:pdf},
journal = {arXiv preprint arXiv:1409.4011},
title = {{Raiders of the lost architecture: Kernels for Bayesian optimization in conditional parameter spaces}},
year = {2014}
}
@article{gal2016uncertainty,
author = {Gal, Yarin},
file = {:home/mjhutchinson/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gal - Unknown - Uncertainty in deep learning.pdf:pdf},
title = {{Uncertainty in deep learning}},
year = {2017}
}

@misc{richtalkBNNscale,
	author = {Turner, Richard},
	howpublished = {Personal Communication},
	location = {University of Cambridge},
	month = {May},
	year = {2019},
}
