%!TEX root = ../thesis.tex
%*******************************************************************************
%*********************************** Sixth Chapter *****************************
%*******************************************************************************

\chapter{Conclusion}
\label{ch:future_study}

\ifpdf
    \graphicspath{{Chapter6/Figs/Raster/}{Chapter6/Figs/PDF/}{Chapter6/Figs/}}
\else
    \graphicspath{{Chapter6/Figs/Vector/}{Chapter6/Figs/}}
\fi

Presented here are two main groups of contributions. 

First, a detailed study of the effects of various hyper-parameters on the performance of MLP structure BNNs, and identification of systematic trends. We find that layer width and depth are both important factors. Networks that are too large can be detrimental to performance. The effect of the amount of data in the training set has on model optimisation is also investigated. It is shown that the balance between model size and data size is important in controlling the under-fitting or over-fitting of models. Finally the effects of pruning in VI BNNs investigated, and the manner in which the optimiser makes the trade off between reconstruction loss and prior fit terms. We find that in larger networks, many of the weights are pruned completely out the model, reducing the model to a minimum descriptive length, or in some cases further.

Second, is a new method for searching for optimal MLP structure networks in Bayesian Neural Networks, a task currently untackled. This method robustly outperforms the random search on the majority of tasks, excepting cases where the noise from randomness in training BNNs is too great. In most tasks it is as good as, or marginally better than simply performing Bayesian optimisation over the final performance of the BNNs.

\section{Future study}

This thesis represents a first step into architecture search for BNNs. Two directions present themselves as interesting avenues for future work. 

First, a continuation of this specific search method for architecture search, or for hyper-parameter optimisation in general. The ability to incorporate information from snapshots of time during training to allow for early stopping is a reasonably uncommon property in search mechanisms, and this method presented here is entirely novel.

Several immediate improvements could be investigated. The current acquisition functions used are inherently greedy algorithms. Trading off for some additional exploration may help the search rapidly identify good regions to explore. \citet{Brochu2010a} investigates methods for altering the exploration-exploitation balance in the acquisition functions used in this project. \citet{Ginsbourger2008} introduces methodology to do parallel processing EI Bayesian optimisation. Instead of the stochastic noise introduced in this project to diversify the selected search points, this work introduces multi-step optimal acquisition functions that may provide significant gains in performance of this model. \citet{Hennig2012} proposes a new acquisition function attempts to minimise the entropy of the predictive models posterior. The objective of this is to optimally find the \textit{location} of the maximum, not necessarily explore it. Application of this to the search space may speed up the process of finding the location of optimal architectures.

The other line of investigation is to apply some of the more modern architecture search techniques to BNNs. Recent efforts have scaled BNNs to the size of some larger CNN models \cite{Shridhar2018,richtalkBNNscale}, and so automated search becomes even more important at this scale. It is not immediately clear how to apply network morphisms or one-shot modelling to BNNs, key components in efficient large scale search, but the results of this would be intriguing to investigate.